{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38976b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed26874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gflasso(y, lams, verbose=0):\n",
    "    \"\"\"\"\"\"\n",
    "    n, p = y.shape\n",
    "    \n",
    "    mean_signal = np.mean(y, axis=0)\n",
    "    w = default_weights(n)\n",
    "\n",
    "    lams = sorted(lams, key=lambda x: -x)\n",
    "    nlambda = len(lams)\n",
    "    C = left_multiply_by_xt(y, w)\n",
    "    \n",
    "    beta = []\n",
    "    active_set = []\n",
    "    res = {}\n",
    "    for lam in lams:\n",
    "        # Optimize for this lambda with a warm restart from the previous lambda\n",
    "        output = optimize(beta, active_set, lam, C, y, w, verbose=verbose)\n",
    "        res[lam] = output.copy()\n",
    "        \n",
    "    return res\n",
    "\n",
    "def onetwo_norm(A):\n",
    "    return np.sqrt(np.sum(A**2, axis=0)).sum()\n",
    "\n",
    "def default_weights(n):\n",
    "    \"\"\"Generate default weights according to Eq.(5) in Bleakley & Vert, 2011 \"\"\"\n",
    "    a = 1 + np.arange(n-1)\n",
    "    w = np.sqrt(n / (a * (n-a)))\n",
    "    return w\n",
    "\n",
    "\n",
    "def left_multiply_by_xt(Y, w):\n",
    "    \"\"\"Fast computation for X'*Y for the group fused Lasso\"\"\"\n",
    "    n, p = Y.shape\n",
    "    u = np.cumsum(Y, axis=0)\n",
    "    part1 = np.arange(1,n).reshape(-1,1) @ u[-1].reshape(1,-1) / n - u[:-1]\n",
    "    part2 = np.array([w,]*p).transpose()\n",
    "    C = np.multiply(part1, part2)\n",
    "    return C\n",
    "\n",
    "\n",
    "def reconstruct_signal(n, act_set, beta, w, meansignal):\n",
    "    \"\"\"Reconstruct a piecewise constant profile from its increments\"\"\"\n",
    "    \n",
    "    if meansignal is None:\n",
    "        meansignal = np.zeros((1,p))\n",
    "    \n",
    "    if len(beta) == 0:\n",
    "        return np.array([meansignal]*n)\n",
    "    \n",
    "    a, p = beta.shape \n",
    "    # a: number of change points\n",
    "    # p: dimension of signal\n",
    "    \n",
    "    signal = np.zeros((n, p))\n",
    "    weights = np.array([w[act_set.reshape(-1,1) - 1]] * p).transpose()\n",
    "    signal[act_set.flatten()] = np.multiply(beta, weights).reshape(-1, p)\n",
    "    signal = np.cumsum(signal, axis=0)\n",
    "    \n",
    "    # set the mean signal to meansignal\n",
    "    m = meansignal - np.mean(signal, axis=0)\n",
    "    signal += np.array([m]*n)\n",
    "    \n",
    "    return signal\n",
    "\n",
    "\n",
    "def xtx(n, A, B, w):\n",
    "    \"\"\"\n",
    "    Compute X[:, A].T * X[:, B], where X is the design matrix of the weighted GFL\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    n:\n",
    "    A: a column vector of indices in [0, n-2]\n",
    "    B: a column vector of indices in [0, n-2]\n",
    "    w: (n-1) * 1 column vector of weights\n",
    "    \"\"\"\n",
    "    \n",
    "    assert type(A) is np.ndarray, f\"arg A: {A} is not np.ndarray!\"\n",
    "    assert type(B) is np.ndarray, f\"arg B: {B} is not np.ndarray!\"\n",
    "    \n",
    "    u = np.array([A+1]*len(B)).transpose().reshape(len(A), len(B))\n",
    "    v = np.array([B+1]*len(A)).reshape(len(A), len(B))\n",
    "    tmp = np.multiply(n-np.maximum(u,v), w[A] @ w[B].T)/n\n",
    "    g = np.multiply(np.minimum(u, v), tmp)\n",
    "    return g\n",
    "    \n",
    "    \n",
    "def multiplyXtXbysparse(n, active_set, beta, w):\n",
    "    \"\"\"\"\"\"\n",
    "    a, p = beta.shape\n",
    "    \n",
    "    if a == 0:\n",
    "        return np.zeros((n-1, p))\n",
    "    else:\n",
    "        # First multiply beta by the weights\n",
    "        beta = np.multiply(beta, np.array([w[active_set]]*p).transpose().reshape(-1, p))\n",
    "        # compute the matrix s of increments of r\n",
    "        s = np.zeros((n-1, p))\n",
    "        s[active_set.flatten(),:] = beta.copy()\n",
    "        s = np.cumsum(s[::-1,:], 0)[::-1,:]\n",
    "        u = (active_set.flatten() + 1) @ beta\n",
    "        s = s - np.array([u]*(n-1)) / n\n",
    "        r = np.cumsum(s, 0)\n",
    "        # then multiply the rows by the weights\n",
    "        r = np.multiply(r, np.array([w]*p).transpose())\n",
    "\n",
    "    return r\n",
    "\n",
    "\n",
    " \n",
    "def block_coordinate_descent(beta, active_set, xty, n, w, lam, verbose=1):\n",
    "    \"\"\"Block coordinate descent over the active set of beta\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    beta: (np.array): a by p matrix as initializer\n",
    "    active_set: (array): a by 1 matrix with indices of nonzero beta_t, in the range of [1, T-1]\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "            \n",
    "    \"\"\"\n",
    "    maxit = 1e5\n",
    "    \n",
    "    if len(beta) == 0:\n",
    "        return beta\n",
    "    \n",
    "    a, p = beta.shape\n",
    "    tol = 1e-8 * p\n",
    "    norm_beta = np.linalg.norm(beta, axis=1)\n",
    "    gain = 2*tol*np.ones(a);\n",
    "    \n",
    "    itc = 0\n",
    "    while(any(gain > tol) & (itc < maxit)):\n",
    "        i = np.mod(itc, a)\n",
    "        asi = active_set[i]\n",
    "\n",
    "        # Compute the vector S\n",
    "        XitX = xtx(n, asi, active_set, w)\n",
    "        gammai = XitX.flatten()[i]\n",
    "        # the indices of the active set excluding the one being optimized. This could be empty\n",
    "        indwithouti = np.array([k for k in range(a) if k != i])\n",
    "        if len(indwithouti) == 0:\n",
    "            S = xty[i, :]\n",
    "        else:\n",
    "            S = xty[i, :] - XitX.flatten()[indwithouti] @ beta[indwithouti, :]\n",
    "        nS = np.linalg.norm(S)\n",
    "        if nS < lam:\n",
    "            newbeta = np.zeros(p) # If |S| < lambda, then softshriholding returns zero\n",
    "        else:\n",
    "            newbeta = (S * (1 - lam / nS) / gammai).flatten() # Otherwise, shrink S following eq.(9)\n",
    "        \n",
    "        assert type(newbeta) is np.ndarray, \"newbeta is empty\"\n",
    "        \n",
    "        # compute the gain in the objective funtion at this iteration\n",
    "        new_norm_beta = np.linalg.norm(newbeta)\n",
    "\n",
    "        gain[i] = (gammai*(norm_beta[i] + new_norm_beta)/2 + lam)*(norm_beta[i]-new_norm_beta) + \\\n",
    "                    S @ (newbeta - beta[i,:])\n",
    "\n",
    "        if verbose > 1:\n",
    "            logging.info(f\"[BCD] Iter {itc}, update block {asi}, gain={gain[i]}\\n\")\n",
    "\n",
    "        # update beta\n",
    "        beta[i, :] = newbeta.copy()\n",
    "        norm_beta[i] = new_norm_beta\n",
    "\n",
    "        itc += 1\n",
    "            \n",
    "    return beta\n",
    "\n",
    "\n",
    "def optimize(beta, active_set, lam, xty, y, w, verbose=1):\n",
    "    \"\"\"Solve the group fused Lasso optimization problem\"\"\"\n",
    "\n",
    "    tol = 1e-8\n",
    "    n, p = xty.shape\n",
    "    n += 1\n",
    "    solved = False\n",
    "    \n",
    "    t_start = time.time()\n",
    "    \n",
    "    itc = 1\n",
    "    meansignal = np.mean(y, axis=0)\n",
    "    res = {}\n",
    "    res['obj'] = np.zeros(10000)\n",
    "    \n",
    "    while not solved:\n",
    "        # optimize gfl given current active set\n",
    "        beta = block_coordinate_descent(beta, active_set, xty[list(active_set), :], n, w, lam, verbose=verbose-1)\n",
    "        \n",
    "        # update active set\n",
    "        \n",
    "        if len(beta) == 0:\n",
    "            S = -xty # dim: n-1 x p\n",
    "            normS = np.sum(S**2, axis=1) # dim n-1\n",
    "        else:\n",
    "            nonzero_coef = (np.sum(beta**2, axis=1) !=0)\n",
    "            active_set = active_set[nonzero_coef,:]\n",
    "            beta = beta[nonzero_coef, :]\n",
    "\n",
    "            # Check optimality\n",
    "            S = multiplyXtXbysparse(n, active_set, beta, w) - xty\n",
    "            normS = np.sum(S**2, axis=1)\n",
    "        \n",
    "        if len(active_set) == 0:\n",
    "            maxnormS = np.max(normS)\n",
    "            imax = np.argmax(normS)\n",
    "            if maxnormS < lam**2 + tol:\n",
    "                solved = True\n",
    "            else:\n",
    "                # this should only occur in the first iteration\n",
    "                active_set = np.array([[imax]])\n",
    "                beta = np.zeros((1, p))\n",
    "        else:\n",
    "            # for i in AS: normS[i] = lam^2\n",
    "            # for i not in AS: normS[i] < lam^2\n",
    "            \n",
    "            lagr = max(normS[active_set])\n",
    "            lagr = min(lagr, lam**2)\n",
    "            nonas = np.setdiff1d(np.arange(n-1), active_set)\n",
    "            yy = np.max(normS[nonas])\n",
    "            i = np.argmax(normS[nonas])\n",
    "            \n",
    "            if (len(nonas) == 0 or yy < lagr + tol):\n",
    "                # optimality conditions are met; we found the global solution\n",
    "                solved = True\n",
    "                if verbose > 1:\n",
    "                    logging.info(\"Optimality conditions are met!!!\")\n",
    "            else:\n",
    "                # Otherwise we add the block that violates most the optimality condition\n",
    "                active_set = np.vstack((active_set, nonas[i]))\n",
    "                beta = np.vstack((beta, np.zeros((1,p))))\n",
    "        \n",
    "        X = reconstruct_signal(n, active_set, beta, w, meansignal) # this is U in paper\n",
    "        XE = X[1:, :] - X[:-1, :]\n",
    "        \n",
    "        res['obj'][itc] = 0.5 * np.linalg.norm(X-y)**2 + lam * onetwo_norm(XE.T)\n",
    "        res['beta'] = beta\n",
    "        res['active_set'] = active_set\n",
    "        res['X'] = X\n",
    "        \n",
    "        if verbose > 1:\n",
    "            logging.info(f\"\"\"\n",
    "            [optimize] Iteration {itc}\\n\n",
    "            beta:\\n {beta} \\n \n",
    "            active_set:\\n {active_set}\\n \n",
    "            Obj: {res['obj'][itc]} \\n\n",
    "            -----------------------------------\n",
    "            \"\"\"\n",
    "            )\n",
    "        itc += 1\n",
    "        \n",
    "    t_end = time.time() - t_start\n",
    "    if verbose > 1:\n",
    "        logging.info(f\"Total time: {t_end}\")\n",
    "    return res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
