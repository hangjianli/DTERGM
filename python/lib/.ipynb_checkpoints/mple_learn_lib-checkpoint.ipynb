{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09562e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import utils\n",
    "from gfl import *\n",
    "from scipy.linalg import block_diag\n",
    "from functools import partial\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b359f5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STERGMGraph:\n",
    "    def __init__(\n",
    "        self,\n",
    "        lam,\n",
    "        admm_alpha=100,\n",
    "        rel_tol=1e-7,\n",
    "        max_steps=100,\n",
    "        newton_max_steps=100,\n",
    "        converge_tol=1e-7,\n",
    "        gd_lr=0.001,\n",
    "        gd_epochs=500,\n",
    "        gfl_tol=1e-6,\n",
    "        gfl_maxit=1000,\n",
    "        solver=\"newton\",\n",
    "        verbose=1\n",
    "    ):\n",
    "\n",
    "        self.admm_alpha = admm_alpha\n",
    "        self.rel_tol = rel_tol\n",
    "        self.converge_tol = converge_tol\n",
    "        self.max_steps = max_steps\n",
    "        self.newton_max_steps = newton_max_steps\n",
    "        self.verbose = verbose\n",
    "        self.lam = lam\n",
    "        self.lr = gd_lr\n",
    "        self.gfl_tol = gfl_tol,\n",
    "        self.gfl_maxit = gfl_maxit\n",
    "        self.epochs = gd_epochs\n",
    "        self.solver = solver\n",
    "        self.mu = None\n",
    "\n",
    "    def load_data(self, h, y, t, p):\n",
    "        self.H = h #  T x E x p\n",
    "        self.y = y\n",
    "        self.t = t\n",
    "        self.p = p\n",
    "\n",
    "\n",
    "    def mple_solution_path(self, lam_range=[0.01, 100], bins=50, solver='newton'):\n",
    "        lambdas = np.exp(np.linspace(np.log(lam_range[1]), np.log(lam_range[0]), bins))\n",
    "        bic = np.zeros(bins)\n",
    "        best_lambda = None\n",
    "        theta_warmstart = None\n",
    "        for i, lam in enumerate(lambdas):\n",
    "            self.lam = lam\n",
    "            pre_res = theta_warmstart\n",
    "            print(f\"[INFO] lambda = {lam}\")\n",
    "\n",
    "\n",
    "        res = self.mple(initial_values=pre_res, solver=solver)\n",
    "        # self.compute_bic(res)\n",
    "        # res['bic']\n",
    "        # to be continued\n",
    "\n",
    "    def mple(self, initial_values=None, solver='newton', tau_inc=2, tau_dec=2, m=10):\n",
    "        \"\"\"Maximum pseudo-likelihood estimation of theta via ADMM\"\"\"\n",
    "\n",
    "        if initial_values:\n",
    "            theta = initial_values['theta']\n",
    "            z = initial_values['z']\n",
    "            u = initial_values['u']\n",
    "        else:\n",
    "            theta = np.random.normal(loc=0, scale=0.2, size=(self.t, self.p))\n",
    "            u = np.zeros((self.t, self.p))\n",
    "            z = np.random.normal(loc=0, scale=0.2, size=(self.t, self.p))\n",
    "\n",
    "        converged = self.converge_tol + 1.0\n",
    "        steps = 0\n",
    "\n",
    "        # while not converged\n",
    "        while converged > self.converge_tol and steps < self.max_steps:\n",
    "            if self.verbose:\n",
    "                print(f\"[INFO] ADMM step #{steps}\")\n",
    "                print(\"[INFO] Updating theta...\")\n",
    "\n",
    "            # update theta\n",
    "            if solver == 'newton':\n",
    "                start = time.time()\n",
    "                theta = self.theta_update(z, u, theta)\n",
    "                end = time.time()\n",
    "                if self.verbose:\n",
    "                    print(f\"[INFO] Newton converged in: {end - start}.\")\n",
    "            else:\n",
    "                start = time.time()\n",
    "                theta = self.theta_gd_update(z, u, theta)\n",
    "                end = time.time()\n",
    "                if self.verbose:\n",
    "                    print(f\"[INFO] Gradient descent converged in: {end - start}.\")\n",
    "\n",
    "            if self.verbose:\n",
    "                print(\"[INFO] Updating z...\")\n",
    "\n",
    "            # update z by solving group fused lasso\n",
    "            z_old = np.copy(z)\n",
    "            U = theta + u  # observed singal, T x p\n",
    "            _lam = self.lam\n",
    "\n",
    "            assert type(U) is np.ndarray\n",
    "            start = time.time()\n",
    "            res = gflasso(U, [_lam], verbose=self.verbose) # need to rewrite lam format (TODO)\n",
    "            end = time.time()\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"[INFO] Group fused Lasso converged in: {end - start}.\")\n",
    "            z = res[_lam]['X']  # T x p # TODO\n",
    "            assert z.shape == (self.t, self.p)\n",
    "\n",
    "            dual_residual = z - z_old\n",
    "            primal_residual = theta - z\n",
    "\n",
    "            if self.verbose > 0:\n",
    "                print(\"[INFO] Updating u...\")\n",
    "\n",
    "            # update u\n",
    "            u += primal_residual\n",
    "\n",
    "            primal_resnorm = np.sqrt(np.mean(primal_residual.flatten() ** 2))\n",
    "            dual_resnorm = np.sqrt(np.mean(dual_residual.flatten() ** 2))\n",
    "\n",
    "            converged = max(primal_resnorm, dual_resnorm)\n",
    "\n",
    "            if primal_resnorm > dual_resnorm * m:\n",
    "                self.admm_alpha *= tau_inc\n",
    "                u /= tau_inc\n",
    "                if self.verbose > 0:\n",
    "                    print(\"-------------------------------------------------\")\n",
    "                    print(f\"[INFO] increasing alpha to {self.admm_alpha}\")\n",
    "            elif dual_resnorm > primal_resnorm * m:\n",
    "                self.admm_alpha /= tau_dec\n",
    "                u *= tau_dec\n",
    "                if self.verbose > 0:\n",
    "                    print(\"-------------------------------------------------\")\n",
    "                    print(f\"[INFO] decreasing alpha to {self.admm_alpha}\")\n",
    "\n",
    "            if self.verbose > 0:\n",
    "                print(f\"[INFO] max mu :  {np.max(self.mu)}\")\n",
    "                print(f\"[INFO] dual_resnorm: {dual_resnorm:.6f}\")\n",
    "                print(f\"[INFO] primal_resnorm: {primal_resnorm:.6f}\")\n",
    "                print(f\"[INFO] convergence: {converged:.6f}\")\n",
    "                print(\"-------------------------------------------------\")\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"[INFO] ADMM finished!\")\n",
    "\n",
    "        return {'theta': theta, 'z': z, 'u': u}\n",
    "\n",
    "\n",
    "    def compute_bic(self, est_chapts):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def theta_update(self, z, u, theta):\n",
    "        \"\"\"Update theta via Newton method\"\"\"\n",
    "\n",
    "        g = partial(self.theta_update_grad_f, z, u)\n",
    "        h = partial(self.theta_update_hess_f)\n",
    "\n",
    "        converged = self.rel_tol + 1.\n",
    "        steps = 0\n",
    "\n",
    "        while converged > self.rel_tol and steps < self.newton_max_steps:\n",
    "            if self.verbose > 1:\n",
    "                print(f\"[INFO] Inner Step # {steps}. Current diff is {converged}\")\n",
    "\n",
    "            hess_f = h(theta)\n",
    "            grad_f = g(theta)\n",
    "\n",
    "            # delta_theta = (-np.linalg.inv(hess_f) @ grad_f).reshape(theta.shape)\n",
    "            # diagonal hessian\n",
    "            hess = np.diagonal(hess_f)\n",
    "            delta_theta = (-grad_f / hess).reshape(theta.shape)\n",
    "\n",
    "            converged = np.abs(np.sum(delta_theta.flatten()) / 2.)\n",
    "            if converged <= self.rel_tol:\n",
    "                break\n",
    "\n",
    "            theta += delta_theta\n",
    "\n",
    "            if self.verbose > 1:\n",
    "                print(f\"[INFO] Loss = {self.loss_lr(theta, z, u)}\")\n",
    "                print(f\"max of mu is {np.max(self.mu)}\")\n",
    "            steps += 1\n",
    "\n",
    "        return theta  # T x p\n",
    "\n",
    "    def theta_update_hess_f(self, theta):\n",
    "        \"\"\"Update the Hessian in Newton's step\n",
    "\n",
    "        theta: T x p\n",
    "\n",
    "        \"\"\"\n",
    "        Z = np.sum(self.H * theta[:, np.newaxis, :], axis=2).squeeze()  # compute log odds Zhat: T x E x 1\n",
    "        if self.verbose > 2:\n",
    "            print(f\"[INFO] Z(TxE): \\n {utils.pretty_str(Z, 3)}\")\n",
    "            print(f\"[INFO] H(TExTp): \\n {utils.pretty_str(self.H, 3)}\")\n",
    "        mu = sigmoid(Z)  # mu: T x E\n",
    "        W = mu * (1 - mu)  # W: T x E\n",
    "        _hess1 = self.H.transpose(0, 2, 1) * W[:, np.newaxis, :]\n",
    "        _hess2 = np.zeros((self.t, self.p, self.p))\n",
    "        for i in range(self.t):\n",
    "            _hess2[i, :, :] = _hess1[i, :, :] @ self.H[i, :, :]\n",
    "        hess = block_diag(*_hess2) + self.admm_alpha * np.identity(self.p * self.t)\n",
    "        self.mu = mu\n",
    "\n",
    "        return hess  # Tp x Tp\n",
    "\n",
    "    def theta_gd_update(self, z, u, theta):\n",
    "        \"\"\"Update theta via gradient descent \"\"\"\n",
    "        loss = []\n",
    "        g = partial(self.theta_update_grad_f, z, u)\n",
    "\n",
    "        for it in range(self.epochs):\n",
    "            delta_theta = self.lr * g(theta).reshape(theta.shape)\n",
    "            converged = np.abs(np.sum(delta_theta.flatten()) / 2.)\n",
    "            if converged < self.rel_tol:\n",
    "                if self.verbose:\n",
    "                    print(f\"Gradient descent converged. Epoch number {it}\")\n",
    "                break\n",
    "            theta -= delta_theta\n",
    "            curloss = self.loss_lr(theta, z, u)\n",
    "            if self.verbose > 2:\n",
    "                print(f\"[INFO] Loss: {curloss: .5f}\")\n",
    "            loss.append(curloss)\n",
    "        self.loss = loss\n",
    "\n",
    "        return theta\n",
    "\n",
    "    def theta_update_grad_f(self, z, u, theta):\n",
    "        \"\"\"Update the gradient in Newton step \"\"\"\n",
    "\n",
    "        Z = np.sum(self.H * theta[:, np.newaxis, :], axis=2).squeeze()  # compute log odds Zhat: T x E x 1\n",
    "        self.mu = sigmoid(Z)  # mu: T x E\n",
    "        if self.verbose > 1:\n",
    "            print(f\"[INFO] max mu : \\n {np.max(self.mu)}\")\n",
    "        # y = self.X.reshape(self.t, -1)  # T x E\n",
    "        pnlty = self.admm_alpha * (theta - z + u)  # T x p\n",
    "        grad = - np.sum(self.H * (self.y - self.mu)[:, :, np.newaxis], axis=1).squeeze() + pnlty\n",
    "\n",
    "        return grad.flatten()  # 1d Tp\n",
    "\n",
    "    def loss_lr(self, theta, z, u):\n",
    "        \"\"\"First objective function (LR) in ADMM\"\"\"\n",
    "        # y = self.X.reshape(self.t, -1)  # T x E\n",
    "        # negative pseudo log-likelihood\n",
    "        predict_1 = self.y * np.log(self.mu)\n",
    "        predict_0 = (1 - self.y) * np.log(1 - self.mu)\n",
    "        penalty = np.sum((theta - z + u) ** 2)\n",
    "\n",
    "        return -np.sum(predict_1 + predict_0) + self.admm_alpha / 2 * penalty\n",
    "\n",
    "\n",
    "def compute_mle():\n",
    "    pass\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + safe_exp(-x))\n",
    "\n",
    "\n",
    "def safe_exp(x):\n",
    "    return np.exp(x.clip(-50., 50.))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
